{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Applications/PyCharm.app/Contents/plugins/python/helpers-pro/jupyter_debug', '/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev', '/Users/david/Documents/mastersCode/ubm/data/pitch_tracking_database', '/Users/david/.conda/envs/ubm/lib/python310.zip', '/Users/david/.conda/envs/ubm/lib/python3.10', '/Users/david/.conda/envs/ubm/lib/python3.10/lib-dynload', '', '/Users/david/.conda/envs/ubm/lib/python3.10/site-packages', '/Users/david/Documents/mastersCode/ubm', '/Users/david/Documents/mastersCode/ubm']\n"
     ]
    }
   ],
   "source": [
    "# create pickles of ads with features extracted\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/david/Documents/mastersCode/ubm\")\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'feature_extraction.fe_spafe.fe_spafe' from '/Users/david/Documents/mastersCode/ubm/feature_extraction/fe_spafe/fe_spafe.py'>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for gmm-ubm on whale calls\n",
    "from importlib import reload\n",
    "import audio_datastore.audio_datastore as myads\n",
    "reload(myads)\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import config\n",
    "import librosa\n",
    "import utils as my_utils\n",
    "import feature_extraction.fe_spafe.fe_spafe as fe_spafe\n",
    "import feature_extraction.fe_base as fe_base\n",
    "import os.path\n",
    "reload(fe_base)\n",
    "reload(fe_spafe)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# create ads\n",
    "\n",
    "# female ads\n",
    "datasetFolder = r\"/Users/david/Documents/data/speech/ivectors/SPEECH DATA/FEMALE/MIC\"\n",
    "female_ads = myads.AudioDatastore()\n",
    "female_ads.populate(datasetFolder, include_sub_folders=True, label_source=True)\n",
    "Counter(female_ads.labels).values()\n",
    "# male ads\n",
    "datasetFolder = r\"/Users/david/Documents/data/speech/ivectors/SPEECH DATA/MALE/MIC\"\n",
    "male_ads = myads.AudioDatastore()\n",
    "male_ads.populate(datasetFolder, include_sub_folders=True, label_source=True)\n",
    "Counter(male_ads.labels).values()\n",
    "# combine ads\n",
    "ads = myads.AudioDatastore()\n",
    "ads.set([datasetFolder], files=female_ads.files + male_ads.files, labels=female_ads.labels + male_ads.labels)\n",
    "# split ads\n",
    "all_labels = [\"M01\", \"M02\", \"M03\", \"M04\", \"M06\", \"M07\", \"M08\", \"M09\", \"F01\", \"F02\", \"F03\", \"F04\", \"F06\", \"F07\",\n",
    "                     \"F08\", \"F09\", \"M05\", \"M10\", \"F05\", \"F10\"]\n",
    "\n",
    "ads_all = myads.subset(ads, all_labels)\n",
    "ads_all, _ = myads.split(ads_all, 30)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['F01', 'F02', 'F03', 'F04', 'F05', 'F06', 'F07', 'F08', 'F09', 'F10', 'M01', 'M02', 'M03', 'M04', 'M05', 'M06', 'M07', 'M08', 'M09', 'M10'])\n",
      "dict_values([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(ads_all.labels).keys()) # equals to list(set(words))\n",
    "print(Counter(ads_all.labels).values()) # counts the elements' frequency"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/david/.cache/torch/hub/snakers4_silero-vad_master\n"
     ]
    }
   ],
   "source": [
    "# define torch transforms for normal file conversion\n",
    "\n",
    "import my_torch.tuts2.torch_transforms as torch_t\n",
    "import my_torch.tuts2.combo_torch_transforms as torch_c\n",
    "reload(torch_t)\n",
    "reload(torch_c)\n",
    "\n",
    "classic_process = torch_c.file_to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import feature_extraction.fe_base\n",
    "# fe methods\n",
    "from feature_extraction.fe_configs import NormFactor\n",
    "from spafe.features.mfcc import *\n",
    "reload(fe_base)\n",
    "\n",
    "def get_mfcc(sign):\n",
    "    features = mfcc(sig=sign, fs=config.SAMPLING_RATE, num_ceps=config.N_MFCC, nfft=config.N_FFT)\n",
    "    features = zero_handling(features)\n",
    "    return features\n",
    "\n",
    "def get_log_mel(sign):\n",
    "    features, _ = mel_spectrogram(sig=sign, fs=config.SAMPLING_RATE, nfft=config.N_FFT)\n",
    "    features = zero_handling(features)\n",
    "    log_features = np.log(features)\n",
    "    return log_features\n",
    "\n",
    "def get_delta_mfccs(sign):\n",
    "    features = get_mfcc(sign)\n",
    "    delta_features = librosa.feature.delta(features.T, order=1).T\n",
    "    features_comb = np.concatenate([features, delta_features], axis=1)\n",
    "    return features_comb\n",
    "\n",
    "class FeatureExtractorMfcc(fe_base.FeatureExtractorBase):\n",
    "\n",
    "    def __init__(self, normalize=False):\n",
    "        super().__init__(normalize)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'mfcc'\n",
    "\n",
    "    def extract_feature(self, sig):\n",
    "        features = mfcc(sig=sig, fs=config.SAMPLING_RATE, num_ceps=config.N_MFCC, nfft=config.N_FFT)\n",
    "        features = zero_handling(features)\n",
    "        return features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed files:  10\n",
      "processed files:  20\n",
      "average power:  0.016570018892731627\n"
     ]
    }
   ],
   "source": [
    "# quick test - use ads for normalisation and average power if augmenting or adding noise\n",
    "import matplotlib.pyplot as plt\n",
    "import spafe.utils.vis as vis\n",
    "import torch\n",
    "\n",
    "ads_normalize, _ = myads.split(ads_all, 2) # use one speaker\n",
    "fe_method = FeatureExtractorMfcc()\n",
    "fe_method.set_normalisation(ads_normalize, classic_process, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_process = torch_t.ComposeTransform([\n",
    "    classic_process,\n",
    "    fe_method\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "feature_test = train_process(ads_all.files[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed files: 0/600\n",
      "processed files: 10/600\n",
      "processed files: 20/600\n",
      "processed files: 30/600\n",
      "processed files: 40/600\n",
      "processed files: 50/600\n",
      "processed files: 60/600\n",
      "processed files: 70/600\n",
      "processed files: 80/600\n",
      "processed files: 90/600\n",
      "processed files: 100/600\n",
      "processed files: 110/600\n",
      "processed files: 120/600\n",
      "processed files: 130/600\n",
      "processed files: 140/600\n",
      "processed files: 150/600\n",
      "processed files: 160/600\n",
      "processed files: 170/600\n",
      "processed files: 180/600\n",
      "processed files: 190/600\n",
      "processed files: 200/600\n",
      "processed files: 210/600\n",
      "processed files: 220/600\n",
      "processed files: 230/600\n",
      "processed files: 240/600\n",
      "processed files: 250/600\n",
      "processed files: 260/600\n",
      "processed files: 270/600\n",
      "processed files: 280/600\n",
      "processed files: 290/600\n",
      "processed files: 300/600\n",
      "processed files: 310/600\n",
      "processed files: 320/600\n",
      "processed files: 330/600\n",
      "processed files: 340/600\n",
      "processed files: 350/600\n",
      "processed files: 360/600\n",
      "processed files: 370/600\n",
      "processed files: 380/600\n",
      "processed files: 390/600\n",
      "processed files: 400/600\n",
      "processed files: 410/600\n",
      "processed files: 420/600\n",
      "processed files: 430/600\n",
      "processed files: 440/600\n",
      "processed files: 450/600\n",
      "processed files: 460/600\n",
      "processed files: 470/600\n",
      "processed files: 480/600\n",
      "processed files: 490/600\n",
      "processed files: 500/600\n",
      "processed files: 510/600\n",
      "processed files: 520/600\n",
      "processed files: 530/600\n",
      "processed files: 540/600\n",
      "processed files: 550/600\n",
      "processed files: 560/600\n",
      "processed files: 570/600\n",
      "processed files: 580/600\n",
      "processed files: 590/600\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# ads_train_subset, _ = myads.split(ads_all, 2)\n",
    "all_features = []\n",
    "count = 0\n",
    "for file in ads_all.files:\n",
    "    if count % 10 == 0:\n",
    "        print('processed files: ' + str(count) + '/' + str(len(ads_all.files)))\n",
    "    feature = classic_process(file)\n",
    "    all_features.append(feature)\n",
    "    count = count + 1\n",
    "print('done')\n",
    "ads_all.data = all_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "ads_and_info = {\n",
    "    'ads': ads_all,\n",
    "    'norm': None,\n",
    "    'average_power': None,\n",
    "    'fe_method': None,\n",
    "    'info': 'pitch tracking database, normalised audio data, sped up 30 examples each'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(ads_and_info, open('pitch_tracking_db_normalised_audio_30_v2.pickle','wb'))\n",
    "\n",
    "# adsEnrollSaved = pickle.load(open('adsEnroll.pickle','rb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
