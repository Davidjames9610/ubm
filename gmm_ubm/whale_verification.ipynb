{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test for gmm-ubm on whale calls\n",
    "\n",
    "from importlib import reload\n",
    "import audio_datastore.audio_datastore as myads\n",
    "reload(myads)\n",
    "import os.path\n",
    "from collections import Counter\n",
    "import random\n",
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav\n",
    "from spafe.utils import vis\n",
    "import matplotlib.pyplot as plt\n",
    "import misc.vad as vad\n",
    "from sklearn.mixture import GaussianMixture, _gaussian_mixture\n",
    "reload(vad)\n",
    "import numpy as np\n",
    "import librosa.feature as fe\n",
    "reload(fe)\n",
    "import librosa.util.utils as librosa_utils\n",
    "import whale.setup.constants as const\n",
    "from scipy.special import logsumexp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# [2] feature extraction\n",
    "# \t• Normalize the audio\n",
    "# \t• Use detectSpeech to remove nonspeech regions from the audio\n",
    "# \t• Extract features from the audio\n",
    "# \t• Normalize the features\n",
    "#   * Apply cepstral mean normalization\n",
    "\n",
    "eps = np.finfo(np.float64).eps\n",
    "\n",
    "def helper_feature_extraction(raw_audio_file, norm = None):\n",
    "    # read in file\n",
    "    (signal_rate, signal) = wav.read(raw_audio_file)\n",
    "\n",
    "    # normalise\n",
    "    signal = librosa_utils.normalize(signal)\n",
    "\n",
    "    # detect / vad, not working currently\n",
    "\n",
    "    # fe\n",
    "    mfcc_feats = fe.mfcc(y=signal,sr=const.SAMPLING_RATE, n_mfcc=13, n_fft=1024).T\n",
    "\n",
    "    # feature normalisation and Cepstral mean subtraction (for channel noise)\n",
    "    if norm:\n",
    "        mfcc_feats = (mfcc_feats - norm.means) / norm.std\n",
    "        mfcc_feats = mfcc_feats - np.mean(mfcc_feats)\n",
    "        return mfcc_feats\n",
    "    else:\n",
    "        return mfcc_feats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "datasetFolder = r\"/Users/david/Documents/mastersCode/ubm/whale/setup/data_spliced\"\n",
    "ads = myads.AudioDatastore()\n",
    "ads.populate(datasetFolder,include_sub_folders=True, label_source=True)\n",
    "Counter(ads.labels).values()\n",
    "adsTest, adsTrain = myads.split(ads, 15)\n",
    "adsTrain, _ = myads.split(adsTrain, 13)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# extract all features and use to get feature normalization\n",
    "all_features = []\n",
    "for file in ads.files:\n",
    "    feature = helper_feature_extraction(file)\n",
    "    all_features.append(feature.T)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def get_norm_factors(all_f):\n",
    "    means = []\n",
    "    std = []\n",
    "    for f in all_f:\n",
    "        means.append(np.mean(f, axis=0))\n",
    "        std.append(np.std(f, axis=0))\n",
    "\n",
    "    means = np.array(means)\n",
    "    means = np.mean(means, axis=0)\n",
    "\n",
    "    std = np.array(std)\n",
    "    std = np.mean(std, axis=0)\n",
    "\n",
    "    class NormFactor:\n",
    "        def __init__(self, m, s):\n",
    "            self.means = m\n",
    "            self.std = s\n",
    "\n",
    "    return NormFactor(means, std)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# collect normalization factors\n",
    "import pickle\n",
    "# import helper_functions\n",
    "\n",
    "normFactors = get_norm_factors(all_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# normalize features\n",
    "all_features_norm = []\n",
    "for i in range(len(all_features)):\n",
    "    normalised = (all_features[i] - normFactors.means) / normFactors.std\n",
    "    normalised = normalised - np.mean(normalised)\n",
    "    all_features_norm.append(normalised)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# COLLECT TRAIN FEATURES\n",
    "\n",
    "train_features = []\n",
    "for i in range(len(adsTrain.files)):\n",
    "    train_feature = helper_feature_extraction(adsTrain.files[i], normFactors)\n",
    "    train_features.append(train_feature)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn model-23.525702636759235\n"
     ]
    }
   ],
   "source": [
    "ubm = GaussianMixture(n_components=32, covariance_type='diag')\n",
    "train_features_flattened = np.array([item for sublist in train_features for item in sublist])\n",
    "ubm.fit(train_features_flattened)\n",
    "print('sklearn model' + str(ubm.score(train_features[0])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ENROLL\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
