{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'misc.vad' from 'C:\\\\Users\\\\David\\\\OneDrive\\\\Desktop\\\\pythonProjects\\\\ubm\\\\misc\\\\vad.py'>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import audioDatastore.AudioDatastore as myads\n",
    "reload(myads)\n",
    "import os.path\n",
    "from collections import Counter\n",
    "import random\n",
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav\n",
    "from spafe.utils import vis\n",
    "import matplotlib.pyplot as plt\n",
    "import misc.vad as vad\n",
    "reload(vad)"
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "Steps:\n",
    "[1] Organize data into train, enroll and verify - done\n",
    "[2] Feature Extraction - nomalization not sure about - Apply cepstral mean normalization\n",
    "[3] Init UBM GMM, train with EM\n",
    "[4] MAP Estimate for different speakers\n",
    "[5] Evaluation -"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# [1] separate data into train, enroll and verify\n",
    "\n",
    "datasetFolder = r\"C:\\Users\\David\\OneDrive\\Desktop\\matlab\\data\\speech_commands_v0.01\"\n",
    "main_ads = myads.AudioDatastore()\n",
    "main_ads.populate(datasetFolder,include_sub_folders=True, label_source=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "ads = myads.subset(main_ads, label='stop')\n",
    "\n",
    "speakers = []\n",
    "for file in ads.files:\n",
    "    nm = os.path.basename(file)\n",
    "    nm = nm.split('_')[0]\n",
    "    speakers.append('a' + nm)\n",
    "\n",
    "ads.set(labels=speakers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "num_speakers_to_enroll = 10\n",
    "label_count = Counter(ads.labels)\n",
    "for_enroll_and_test_set = []\n",
    "for key, cnts in list(label_count.items()):\n",
    "    if cnts >= 3:\n",
    "        for_enroll_and_test_set.append(key)\n",
    "for_enroll = random.sample(for_enroll_and_test_set, num_speakers_to_enroll)\n",
    "ads_enroll_and_validate = myads.subset(ads, label=for_enroll)\n",
    "ads_enroll, _ = myads.split(ads_enroll_and_validate, int(len(ads_enroll_and_validate.labels) / 2))\n",
    "ads_test = myads.subset(ads, for_enroll_and_test_set)\n",
    "ads_test = myads.filter(ads_test, ads_enroll.labels)\n",
    "ads_train_ubm = myads.filter(ads, ads_test.labels)\n",
    "ads_train_ubm = myads.filter(ads_train_ubm, ads_enroll.labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# [2] feature extraction\n",
    "# \t• Normalize the audio\n",
    "# \t• Use detectSpeech to remove nonspeech regions from the audio\n",
    "# \t• Extract features from the audio\n",
    "# \t• Normalize the features\n",
    "#   * Apply cepstral mean normalization\n",
    "\n",
    "def helper_feature_extraction(audio_file, norm_factors = None):\n",
    "    # read in file\n",
    "    (rate,sig) = wav.read(audio_file)\n",
    "\n",
    "    # normalise\n",
    "    sig = sig / max(sig)\n",
    "\n",
    "    # detect speech => convert this into one function\n",
    "    v = vad.VoiceActivityDetector(rate, sig, 0.5)\n",
    "    detected_speach = v.detect_speech()\n",
    "    idx = v.convert_windows_to_readible_labels(detected_speach)\n",
    "    if idx:\n",
    "        sig = sig[idx[0]:idx[1]]\n",
    "        mfcc_feat = mfcc(sig,rate)\n",
    "\n",
    "        # feature normalisation and Cepstral mean subtraction (for channel noise)\n",
    "        if norm_factors:\n",
    "            mfcc_feat = (mfcc_feat - norm_factors.means) / norm_factors.std\n",
    "            mfcc_feat = mfcc_feat - np.mean(mfcc_feat)\n",
    "\n",
    "        return mfcc_feat\n",
    "    else:\n",
    "        return []\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "try:\n",
    "    all_features = pickle.load(open('all_features.pickle','rb'))\n",
    "except:\n",
    "    all_features = []\n",
    "    for file in ads.files:\n",
    "        feature = helper_feature_extraction(file)\n",
    "        all_features.append(feature)\n",
    "    pickle.dump(all_features, open('all_features.pickle','wb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.90282935  -0.33093011 -20.11337245 -14.39478988 -20.00294044\n",
      "   1.69950359   2.69763417  -0.9433615   -8.80432994   2.05696167\n",
      "  -4.85549398  -3.99526123  -9.69053713]\n"
     ]
    }
   ],
   "source": [
    "# collect normalization factors, why and where is this used ?\n",
    "\n",
    "import numpy as np\n",
    "# means = np.mean(all_features.fl, axis=0)\n",
    "means = []\n",
    "std = []\n",
    "for feature in all_features:\n",
    "    if len(feature) > 0:\n",
    "        means.append(np.mean(feature, axis=0))\n",
    "        std.append(np.std(feature, axis=0))\n",
    "\n",
    "means = np.array(means)\n",
    "means = np.mean(means, axis=0)\n",
    "\n",
    "std = np.array(std)\n",
    "std = np.std(std, axis=0)\n",
    "\n",
    "class NormFactor:\n",
    "  def __init__(self, m, s):\n",
    "    self.means = m\n",
    "    self.std = s\n",
    "\n",
    "normFactors = NormFactor(means, std)\n",
    "\n",
    "print(normFactors.means)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# normalise features, need to re-look at this ? how does cepstral mean normalization work ?\n",
    "\n",
    "normalised_features = []\n",
    "for i in range(len(all_features)):\n",
    "    if len(all_features[i]) > 0:\n",
    "        normalised = (all_features[i] - means) / std\n",
    "        normalised = normalised - np.mean(normalised)\n",
    "        normalised_features.append(normalised)\n",
    "    else:\n",
    "        normalised_features.append(np.array([]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "# [3] gmm ubm EM\n",
    "\n",
    "# 3.a collect features\n",
    "maxFeatures = 100 # len(ads_train_ubm.files) # len(\n",
    "train_features = []\n",
    "for f in range(maxFeatures):\n",
    "    # extract a feature\n",
    "    train_features.append(helper_feature_extraction(ads_train_ubm.files[f], normFactors))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# 3.b create UBM using sklearn\n",
    "\n",
    "from sklearn.mixture import GaussianMixture, _base, _gaussian_mixture\n",
    "\n",
    "numComponents = 32\n",
    "alpha = np.ones(numComponents) / numComponents\n",
    "numFeatures = normalised_features[0].shape[1]\n",
    "mu = np.random.random((numComponents,numFeatures))\n",
    "sigma = np.random.random((numComponents,numFeatures))\n",
    "\n",
    "ubm = GaussianMixture(n_components=numComponents, covariance_type='diag', weights_init=alpha, means_init=mu, precisions_init=sigma)\n",
    "\n",
    "ubm.means_ = mu\n",
    "ubm.covariances_ = sigma\n",
    "ubm.weights_ = alpha\n",
    "ubm.precisions_cholesky_ = _gaussian_mixture._compute_precision_cholesky(sigma, 'diag')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, log likelihood -5678.877473015325\n",
      "iter: 10, log likelihood -4421.958998248674\n",
      "iter: 20, log likelihood -4406.879645726132\n",
      "iter: 30, log likelihood -4387.929159325731\n",
      "iter: 40, log likelihood -4386.449779104133\n",
      "converged on iter: 45\n"
     ]
    }
   ],
   "source": [
    "# 3.c em algorithm\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "maxIter = 100\n",
    "targetLogLikelihood = 0\n",
    "tol = 1e-3\n",
    "pastL = -np.inf\n",
    "\n",
    "for i in range(maxIter):\n",
    "\n",
    "    # E step\n",
    "    N = np.zeros((1, numComponents))\n",
    "    F = np.zeros((numFeatures, numComponents))\n",
    "    S = np.zeros((numFeatures, numComponents))\n",
    "    L = 0\n",
    "    eps = np.finfo(np.float64).eps\n",
    "\n",
    "    for feature in train_features:\n",
    "        # check for sub par samples\n",
    "        if len(feature) > 0:\n",
    "            logLikelihood = ubm._estimate_weighted_log_prob(feature)\n",
    "            logLikelihoodSum = logsumexp(logLikelihood, axis=1)\n",
    "            gamma = np.exp((logLikelihood.T - logLikelihoodSum)).T\n",
    "\n",
    "            # Compute Baum-Welch stats\n",
    "            n = sum(gamma,1)\n",
    "            f = np.dot(feature.T, gamma)\n",
    "            s = np.dot(feature.T * feature.T, gamma)\n",
    "\n",
    "            N = N + n\n",
    "            F = F + f\n",
    "            S = S + s\n",
    "\n",
    "            L = L + sum(logLikelihoodSum)\n",
    "\n",
    "    # M step\n",
    "    L = L / len(train_features)\n",
    "    if i%10 == 0:\n",
    "        print('iter: ' + str(i) + ', log likelihood ' + str(L))\n",
    "    if L > targetLogLikelihood or np.abs(pastL - L) < tol:\n",
    "        print('converged on iter: ' + str(i))\n",
    "        ubm.converged_ = True\n",
    "        break\n",
    "    else:\n",
    "        pastL = L\n",
    "\n",
    "    N = np.maximum(N, eps)\n",
    "    weights = np.maximum(N / np.sum(N), eps)\n",
    "    newWeights = weights / np.sum(weights)  # why again, because of eps ...\n",
    "    newMeans = F / N\n",
    "    newCovariances = np.maximum((S / N) - np.square(ubm.means_).T, eps)\n",
    "    ubm.weights_ = np.squeeze(newWeights.T)\n",
    "    ubm.means_ = newMeans.T\n",
    "    ubm.covariances_ = newCovariances.T\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn model-36.98687026025601\n",
      "my model-232.77429554284976\n"
     ]
    }
   ],
   "source": [
    "# sklearn does better - probably because of k means init\n",
    "\n",
    "ubm_test = GaussianMixture(n_components=32, covariance_type='diag')\n",
    "train_features_flattened = np.array([item for sublist in train_features for item in sublist])\n",
    "ubm_test.fit(train_features_flattened)\n",
    "print('sklearn model' + str(ubm_test.score(train_features[0])))\n",
    "print('my model' + str(ubm.score(train_features[0])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\miniconda3\\envs\\ubm\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "-20.475179198022968"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enrol\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# for i in len(maxIter):\n",
    "\n",
    "# N = np.zeros(numComponents)\n",
    "# F = np.zeros((numFeatures, numComponents))\n",
    "# S = np.zeros((numFeatures, numComponents))\n",
    "# L = 0\n",
    "# feature = 0\n",
    "#\n",
    "# for i in range(maxIter):\n",
    "#     for f in range(maxFeatures):\n",
    "#         # extract a feature\n",
    "#         feature = helper_feature_extraction(ads_train_ubm.files[f], normFactors)\n",
    "#\n",
    "#         # compute a posteriori log-likelihood\n",
    "#         logLikelihood = helperGMMLogLikelihood(feature,ubm)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}