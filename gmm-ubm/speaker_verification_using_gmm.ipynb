{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'misc.vad' from 'C:\\\\Users\\\\David\\\\OneDrive\\\\Desktop\\\\pythonProjects\\\\ubm\\\\misc\\\\vad.py'>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import audioDatastore.AudioDatastore as myads\n",
    "reload(myads)\n",
    "import os.path\n",
    "from collections import Counter\n",
    "import random\n",
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav\n",
    "from spafe.utils import vis\n",
    "import matplotlib.pyplot as plt\n",
    "import misc.vad as vad\n",
    "from sklearn.mixture import GaussianMixture, _gaussian_mixture\n",
    "reload(vad)"
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "Steps:\n",
    "[1] Organize data into train, enroll and verify - done\n",
    "[2] Feature Extraction - done\n",
    "[3] Init UBM GMM, train with EM - done\n",
    "[4] MAP Estimate for different speakers -\n",
    "[5] Evaluation -"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# [1] separate data into train, enroll and verify\n",
    "\n",
    "datasetFolder = r\"C:\\Users\\David\\OneDrive\\Desktop\\matlab\\data\\speech_commands_v0.01\"\n",
    "main_ads = myads.AudioDatastore()\n",
    "main_ads.populate(datasetFolder,include_sub_folders=True, label_source=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "ads = myads.subset(main_ads, label='stop')\n",
    "\n",
    "speakers = []\n",
    "for file in ads.files:\n",
    "    nm = os.path.basename(file)\n",
    "    nm = nm.split('_')[0]\n",
    "    speakers.append('a' + nm)\n",
    "\n",
    "ads.set(labels=speakers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "num_speakers_to_enroll = 10\n",
    "label_count = Counter(ads.labels)\n",
    "for_enroll_and_test_set = []\n",
    "for key, cnts in list(label_count.items()):\n",
    "    if cnts >= 3:\n",
    "        for_enroll_and_test_set.append(key)\n",
    "for_enroll = random.sample(for_enroll_and_test_set, num_speakers_to_enroll)\n",
    "ads_enroll_and_validate = myads.subset(ads, label=for_enroll)\n",
    "ads_enroll, _ = myads.split(ads_enroll_and_validate, 2)\n",
    "\n",
    "ads_test = myads.subset(ads, for_enroll_and_test_set)\n",
    "ads_test = myads.filter(ads_test, ads_enroll.files) # by file\n",
    "\n",
    "ads_train_ubm = myads.filter(ads, ads_test.files)\n",
    "ads_train_ubm = myads.filter(ads_train_ubm, ads_enroll.files)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# [2] feature extraction\n",
    "# \t• Normalize the audio\n",
    "# \t• Use detectSpeech to remove nonspeech regions from the audio\n",
    "# \t• Extract features from the audio\n",
    "# \t• Normalize the features\n",
    "#   * Apply cepstral mean normalization\n",
    "\n",
    "def helper_feature_extraction(audio_file, norm_factors = None):\n",
    "    # read in file\n",
    "    (rate,sig) = wav.read(audio_file)\n",
    "\n",
    "    # normalise\n",
    "    sig = sig / max(sig)\n",
    "\n",
    "    # detect speech => convert this into one function\n",
    "    v = vad.VoiceActivityDetector(rate, sig, 0.5)\n",
    "    detected_speach = v.detect_speech()\n",
    "    idx = v.convert_windows_to_readible_labels(detected_speach)\n",
    "    if idx:\n",
    "        sig = sig[idx[0]:idx[1]]\n",
    "        mfcc_feat = mfcc(sig,rate)\n",
    "\n",
    "        # feature normalisation and Cepstral mean subtraction (for channel noise)\n",
    "        if norm_factors:\n",
    "            mfcc_feat = (mfcc_feat - norm_factors.means) / norm_factors.std\n",
    "            mfcc_feat = mfcc_feat - np.mean(mfcc_feat)\n",
    "\n",
    "        return mfcc_feat\n",
    "    else:\n",
    "        return []\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# wrapper class for sklearn's gmm\n",
    "\n",
    "class MyGmm:\n",
    "    def __init__(self, num_comp):\n",
    "        self.gmm = GaussianMixture(n_components=num_comp, covariance_type='diag')\n",
    "    def get_gmm(self):\n",
    "        return self.gmm\n",
    "    def set_gmm(self, some_gmm):\n",
    "        self.gmm = some_gmm\n",
    "    def set_gmm_params(self, w, m, c):\n",
    "        # quick check\n",
    "        num_comp = self.gmm.n_components\n",
    "        if w.shape[0]!=num_comp or m.shape[0]!=num_comp or c.shape[0]!=num_comp:\n",
    "            print('somethings wrong')\n",
    "        else:\n",
    "            self.gmm.weights_ = w\n",
    "            self.gmm.means_ = m\n",
    "            self.gmm.covariances_ = c\n",
    "            self.gmm.precisions_cholesky_ = _gaussian_mixture._compute_precision_cholesky(c, 'diag')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "try:\n",
    "    all_features = pickle.load(open('all_features.pickle','rb'))\n",
    "except:\n",
    "    all_features = []\n",
    "    for file in ads.files:\n",
    "        feature = helper_feature_extraction(file)\n",
    "        all_features.append(feature)\n",
    "    pickle.dump(all_features, open('all_features.pickle','wb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.90282935  -0.33093011 -20.11337245 -14.39478988 -20.00294044\n",
      "   1.69950359   2.69763417  -0.9433615   -8.80432994   2.05696167\n",
      "  -4.85549398  -3.99526123  -9.69053713]\n"
     ]
    }
   ],
   "source": [
    "# collect normalization factors, why and where is this used ?\n",
    "\n",
    "import numpy as np\n",
    "# means = np.mean(all_features.fl, axis=0)\n",
    "means = []\n",
    "std = []\n",
    "for feature in all_features:\n",
    "    if len(feature) > 0:\n",
    "        means.append(np.mean(feature, axis=0))\n",
    "        std.append(np.std(feature, axis=0))\n",
    "\n",
    "means = np.array(means)\n",
    "means = np.mean(means, axis=0)\n",
    "\n",
    "std = np.array(std)\n",
    "std = np.std(std, axis=0)\n",
    "\n",
    "class NormFactor:\n",
    "  def __init__(self, m, s):\n",
    "    self.means = m\n",
    "    self.std = s\n",
    "\n",
    "normFactors = NormFactor(means, std)\n",
    "\n",
    "print(normFactors.means)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# normalise features, need to re-look at this ? how does cepstral mean normalization work ?\n",
    "\n",
    "normalised_features = []\n",
    "for i in range(len(all_features)):\n",
    "    if len(all_features[i]) > 0:\n",
    "        normalised = (all_features[i] - means) / std\n",
    "        normalised = normalised - np.mean(normalised)\n",
    "        normalised_features.append(normalised)\n",
    "    else:\n",
    "        normalised_features.append(np.array([]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# [3] gmm ubm EM\n",
    "\n",
    "# 3.a collect features\n",
    "maxFeatures = len(ads_train_ubm.files) # len(\n",
    "train_features = []\n",
    "for f in range(maxFeatures):\n",
    "    # extract a feature\n",
    "    train_features.append(helper_feature_extraction(ads_train_ubm.files[f], normFactors))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# 3.b create UBM using sklearn\n",
    "\n",
    "from sklearn.mixture import GaussianMixture, _base, _gaussian_mixture\n",
    "\n",
    "numComponents = 32\n",
    "alpha = np.ones(numComponents) / numComponents\n",
    "numFeatures = normalised_features[0].shape[1]\n",
    "mu = np.random.random((numComponents,numFeatures))\n",
    "sigma = np.random.random((numComponents,numFeatures))\n",
    "\n",
    "# TODO function to init GMM\n",
    "ubm = GaussianMixture(n_components=numComponents, covariance_type='diag', weights_init=alpha, means_init=mu, precisions_init=sigma)\n",
    "\n",
    "ubm.means_ = mu\n",
    "ubm.covariances_ = sigma\n",
    "ubm.weights_ = alpha\n",
    "ubm.precisions_cholesky_ = _gaussian_mixture._compute_precision_cholesky(sigma, 'diag')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, log likelihood -6664.798061316596\n",
      "iter: 10, log likelihood -5278.89807991309\n",
      "iter: 20, log likelihood -5226.25744729094\n",
      "iter: 30, log likelihood -5205.135872586785\n",
      "iter: 40, log likelihood -5193.571263263209\n",
      "iter: 50, log likelihood -5185.511126132337\n",
      "iter: 60, log likelihood -5181.354714833754\n",
      "iter: 70, log likelihood -5177.664894153763\n",
      "iter: 80, log likelihood -5173.590992279247\n",
      "iter: 90, log likelihood -5170.6458312371715\n"
     ]
    }
   ],
   "source": [
    "# 3.c em algorithm #todo something not right here double check\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "maxIter = 100\n",
    "targetLogLikelihood = 0\n",
    "tol = 1e-3\n",
    "pastL = -np.inf\n",
    "\n",
    "for i in range(maxIter):\n",
    "\n",
    "    # E step\n",
    "    N = np.zeros((1, numComponents))\n",
    "    F = np.zeros((numFeatures, numComponents))\n",
    "    S = np.zeros((numFeatures, numComponents))\n",
    "    L = 0\n",
    "    eps = np.finfo(np.float64).eps\n",
    "\n",
    "    for feature in train_features:\n",
    "        # check for sub par samples\n",
    "        if len(feature) > 0:\n",
    "            logLikelihood = ubm._estimate_weighted_log_prob(feature)\n",
    "            logLikelihoodSum = logsumexp(logLikelihood, axis=1)\n",
    "            gamma = np.exp((logLikelihood.T - logLikelihoodSum)).T\n",
    "\n",
    "            # Compute Baum-Welch stats\n",
    "            n = np.sum(gamma,axis=0)\n",
    "            f = np.dot(feature.T, gamma)\n",
    "            s = np.dot(feature.T * feature.T, gamma)\n",
    "\n",
    "            N = N + n\n",
    "            F = F + f\n",
    "            S = S + s\n",
    "\n",
    "            L = L + np.sum(logLikelihoodSum)\n",
    "\n",
    "    # M step\n",
    "    L = L / len(train_features)\n",
    "    if i%10 == 0:\n",
    "        print('iter: ' + str(i) + ', log likelihood ' + str(L))\n",
    "    if L > targetLogLikelihood or np.abs(pastL - L) < tol:\n",
    "        print('converged on iter: ' + str(i))\n",
    "        ubm.converged_ = True\n",
    "        break\n",
    "    else:\n",
    "        pastL = L\n",
    "\n",
    "    # TODO create function out of this\n",
    "    N = np.maximum(N, eps)\n",
    "    weights = np.maximum(N / np.sum(N), eps)\n",
    "    newWeights = weights / np.sum(weights)  # why again, because of eps ...\n",
    "    newMeans = F / N\n",
    "    newCovariances = np.maximum((S / N) - np.square(ubm.means_).T, eps)\n",
    "    ubm.weights_ = np.squeeze(newWeights.T)\n",
    "    ubm.means_ = newMeans.T\n",
    "    ubm.covariances_ = newCovariances.T\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn model-37.22446257391194\n",
      "my model-248.3206452951756\n"
     ]
    }
   ],
   "source": [
    "# sklearn does better - probably because of k means init\n",
    "\n",
    "ubm_test = GaussianMixture(n_components=32, covariance_type='diag')\n",
    "train_features_flattened = np.array([item for sublist in train_features for item in sublist])\n",
    "ubm_test.fit(train_features_flattened)\n",
    "print('sklearn model' + str(ubm_test.score(train_features[0])))\n",
    "print('my model' + str(ubm.score(train_features[0])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "try:\n",
    "    ubm = pickle.load(open('ubm.pickle','rb'))\n",
    "except:\n",
    "    print('train ubm not avaliable')\n",
    "    pickle.dump(ubm, open('ubm.pickle','wb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker enrollment completed in:  1.7823529243469238\n"
     ]
    }
   ],
   "source": [
    "# [4] map enroll\n",
    "import time\n",
    "\n",
    "t = time.time()\n",
    "# do stuff\n",
    "relevanceFactor = 16\n",
    "speakers = np.unique(ads_enroll.labels)\n",
    "numSpeakers = len(speakers)\n",
    "gmmCellArray = []\n",
    "\n",
    "for i in range(len(speakers)):\n",
    "    adsTrainSubset = myads.subset(ads_enroll, speakers[i])\n",
    "    N = np.zeros((1, numComponents))\n",
    "    F = np.zeros((numFeatures, numComponents))\n",
    "    S = np.zeros((numFeatures, numComponents))\n",
    "\n",
    "    for file in adsTrainSubset.files:\n",
    "        speaker_feature = helper_feature_extraction(file, normFactors)\n",
    "\n",
    "        # BW stats, # TODO make this in to function\n",
    "        logLikelihood = ubm._estimate_weighted_log_prob(speaker_feature)\n",
    "        logLikelihoodSum = logsumexp(logLikelihood, axis=1)\n",
    "        gamma = np.exp((logLikelihood.T - logLikelihoodSum)).T\n",
    "\n",
    "        # Compute Baum-Welch stats\n",
    "        n = np.sum(gamma,axis=0)\n",
    "        f = np.dot(speaker_feature.T, gamma)\n",
    "        s = np.dot(speaker_feature.T * speaker_feature.T, gamma)\n",
    "        l = np.sum(logLikelihoodSum)\n",
    "\n",
    "        N = N + n\n",
    "        F = F + f\n",
    "        S = S + s\n",
    "\n",
    "    N = np.maximum(N, eps)\n",
    "    weights = np.maximum(N / np.sum(N), eps)\n",
    "    newWeights = weights / np.sum(weights)  # why again, because of eps ...\n",
    "    newMeans = F / N\n",
    "    newCovariances = np.maximum((S / N) - np.square(ubm.means_).T, eps)\n",
    "\n",
    "    # create new speaker gmm\n",
    "    gmm = GaussianMixture(n_components=numComponents, covariance_type='diag')\n",
    "    gmm.weights_ = np.squeeze(newWeights.T)\n",
    "    gmm.means_ = newMeans.T\n",
    "    gmm.covariances_ = newCovariances.T\n",
    "    gmm.precisions_cholesky_ = _gaussian_mixture._compute_precision_cholesky(newCovariances.T, 'diag')\n",
    "\n",
    "    # calculate alpha\n",
    "    alpha = N / (N + relevanceFactor)\n",
    "\n",
    "    # adapt the means\n",
    "    gmm.means_ = ((alpha*gmm.means_.T) + ((1-alpha)*ubm.means_.T)).T\n",
    "\n",
    "    # Adapt the variances\n",
    "    sigma = alpha*(S/N) + (1-alpha)*(ubm.covariances_ + np.square(ubm.means_)).T - np.square(gmm.means_).T\n",
    "    gmm.covariances_ = np.maximum(sigma,eps).T\n",
    "\n",
    "    # Adapt the weights\n",
    "    weights = alpha*(N/np.sum(N)) + (1-alpha)*ubm.weights_.T\n",
    "    gmm.weights_ = np.squeeze(weights/np.sum(weights))\n",
    "\n",
    "    gmmCellArray.append(gmm)\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print('speaker enrollment completed in: ', str(elapsed))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "enrolled_gmms = {}\n",
    "for i in range(len(gmmCellArray)):\n",
    "    enrolled_gmms[speakers[i]] = gmmCellArray[i]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# false rejection rate -\n",
    "# The speaker false rejection rate (FRR) is the rate that a given speaker is incorrectly rejected. Use the known speaker set to determine the speaker false rejection rate for a set of thresholds.\n",
    "\n",
    "llr = []\n",
    "\n",
    "for i in range(len(speakers)):\n",
    "    cur_speaker = speakers[i]\n",
    "    local_gmm = enrolled_gmms[cur_speaker]\n",
    "    adsTestSubset = myads.subset(ads_test, cur_speaker)\n",
    "    llrPerSpeaker = np.zeros(len(adsTestSubset.files))\n",
    "\n",
    "    for y in range(len(adsTestSubset.files)):\n",
    "        file = adsTestSubset.files[y]\n",
    "        speaker_feature = helper_feature_extraction(file, normFactors)\n",
    "\n",
    "        logLikelihood = local_gmm._estimate_weighted_log_prob(speaker_feature)\n",
    "        lspeaker = logsumexp(logLikelihood, axis=1)\n",
    "\n",
    "        logLikelihood = ubm._estimate_weighted_log_prob(speaker_feature)\n",
    "        lubm = logsumexp(logLikelihood, axis=1)\n",
    "\n",
    "        llrPerSpeaker[y] = np.mean(lspeaker-lubm)\n",
    "\n",
    "    llr.append(llrPerSpeaker)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}