{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import audioDatastore.AudioDatastore as myads\n",
    "reload(myads)\n",
    "import os.path\n",
    "from collections import Counter\n",
    "import random\n",
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav\n",
    "from spafe.utils import vis\n",
    "import matplotlib.pyplot as plt\n",
    "import misc.vad as vad\n",
    "from sklearn.mixture import GaussianMixture, _gaussian_mixture\n",
    "reload(vad)\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# [2] feature extraction\n",
    "# \t• Normalize the audio\n",
    "# \t• Use detectSpeech to remove nonspeech regions from the audio\n",
    "# \t• Extract features from the audio\n",
    "# \t• Normalize the features\n",
    "#   * Apply cepstral mean normalization\n",
    "\n",
    "eps = np.finfo(np.float64).eps\n",
    "\n",
    "def helper_feature_extraction(raw_audio_file, norm = None):\n",
    "    # read in file\n",
    "    (signal_rate, signal) = wav.read(raw_audio_file)\n",
    "\n",
    "    # normalise\n",
    "    signal = signal / max(signal)\n",
    "\n",
    "    # detect speech => convert this into one function\n",
    "    v = vad.VoiceActivityDetector(signal_rate, signal, 0.5)\n",
    "    detected_speach = v.detect_speech()\n",
    "    idx = v.convert_windows_to_readible_labels(detected_speach)\n",
    "    if idx:\n",
    "        signal = signal[idx[0]:idx[1]]\n",
    "        mfcc_feats = mfcc(signal=signal, samplerate=signal_rate, ceplifter=0, preemph=0, numcep=13, appendEnergy=False)\n",
    "\n",
    "        # feature normalisation and Cepstral mean subtraction (for channel noise)\n",
    "        if norm:\n",
    "            mfcc_feats = (mfcc_feats - norm.means) / norm.std\n",
    "            mfcc_feats = mfcc_feats - np.mean(mfcc_feats)\n",
    "            return mfcc_feats\n",
    "        else:\n",
    "            return mfcc_feats\n",
    "    else:\n",
    "        return []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "Steps:\n",
    "[1] Organize data into train, enroll and verify - done\n",
    "[2] Feature Extraction - done\n",
    "[3] Init UBM GMM, train with EM - done\n",
    "[4] MAP Estimate for different speakers -\n",
    "[5] Evaluation -"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# [1] separate data into train, enroll and verify\n",
    "\n",
    "datasetFolder = r\"C:\\Users\\David\\OneDrive\\Desktop\\matlab\\data\\speech_commands_v0.01\"\n",
    "main_ads = myads.AudioDatastore()\n",
    "main_ads.populate(datasetFolder,include_sub_folders=True, label_source=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "ads = myads.subset(main_ads, label='stop')\n",
    "\n",
    "speakers = []\n",
    "for file in ads.files:\n",
    "    nm = os.path.basename(file)\n",
    "    nm = nm.split('_')[0]\n",
    "    speakers.append('a' + nm)\n",
    "\n",
    "ads.set(labels=speakers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "num_speakers_to_enroll = 10\n",
    "label_count = Counter(ads.labels)\n",
    "for_enroll_and_test_set = []\n",
    "for key, cnts in list(label_count.items()):\n",
    "    if cnts >= 3:\n",
    "        for_enroll_and_test_set.append(key)\n",
    "for_enroll = for_enroll_and_test_set[0:num_speakers_to_enroll]\n",
    "ads_enroll_and_validate = myads.subset(ads, label=for_enroll)\n",
    "ads_enroll, _ = myads.split(ads_enroll_and_validate, 2)\n",
    "\n",
    "ads_test = myads.subset(ads, for_enroll_and_test_set)\n",
    "ads_test = myads.filter(ads_test, ads_enroll.files) # by file\n",
    "\n",
    "ads_train_ubm = myads.filter(ads, ads_test.files)\n",
    "ads_train_ubm = myads.filter(ads_train_ubm, ads_enroll.files)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "reload = False\n",
    "\n",
    "try:\n",
    "    if reload :\n",
    "        raise Exception('reloading')\n",
    "    all_features = pickle.load(open('all_features.pickle','rb'))\n",
    "except:\n",
    "    all_features = []\n",
    "    for file in ads.files:\n",
    "        feature = helper_feature_extraction(file)\n",
    "        all_features.append(feature)\n",
    "    pickle.dump(all_features, open('all_features.pickle','wb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-31.59474313   8.10455255  -2.55111036  -1.90398935  -2.05609326\n",
      "   0.28186454   0.44159752  -0.11896976  -0.65464011  -0.03424841\n",
      "  -0.45836605  -0.54274028  -0.79374816]\n"
     ]
    }
   ],
   "source": [
    "# collect normalization factors\n",
    "\n",
    "import numpy as np\n",
    "# means = np.mean(all_features.fl, axis=0)\n",
    "means = []\n",
    "std = []\n",
    "for feature in all_features:\n",
    "    if len(feature) > 0:\n",
    "        means.append(np.mean(feature, axis=0))\n",
    "        std.append(np.std(feature, axis=0))\n",
    "\n",
    "means = np.array(means)\n",
    "means = np.mean(means, axis=0)\n",
    "\n",
    "std = np.array(std)\n",
    "std = np.mean(std, axis=0)\n",
    "\n",
    "class NormFactor:\n",
    "  def __init__(self, m, s):\n",
    "    self.means = m\n",
    "    self.std = s\n",
    "\n",
    "normFactors = NormFactor(means, std)\n",
    "\n",
    "print(normFactors.means)\n",
    "\n",
    "try:\n",
    "    if reload :\n",
    "        raise Exception('reloading')\n",
    "    normFactors = pickle.load(open('normFactors.pickle','rb'))\n",
    "except:\n",
    "    pickle.dump(normFactors, open('normFactors.pickle','wb'))\n",
    "\n",
    "# mean:, -36.9458, 3.1896, 0.0495, -0.5411, -0.6870, 0.2553, 0.1708, 0.0510, -0.2031, 0.0724, -0.1137, -0.1732, -0.2848\n",
    "# STD: ?\n",
    "# var: 3.9802, 3.4728, 1.9646, 1.3817, 1.1938, 0.8688, 0.8556, 0.7081, 0.6282, 0.538, 0.531, 0.5688, 0.5188\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# normalise features\n",
    "\n",
    "normalised_features = []\n",
    "\n",
    "# save normalised features\n",
    "\n",
    "try:\n",
    "    normalised_features = pickle.load(open('normalised_features.pickle','rb'))\n",
    "    if reload:\n",
    "        raise Exception('reloading')\n",
    "except:\n",
    "    for i in range(len(all_features)):\n",
    "        if len(all_features[i]) > 0:\n",
    "            normalised = (all_features[i] - means) / std\n",
    "            normalised = normalised - np.mean(normalised)\n",
    "            normalised_features.append(normalised)\n",
    "        else:\n",
    "            normalised_features.append(np.array([]))\n",
    "    pickle.dump(normalised_features, open('normalised_features.pickle','wb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero length feature at index: 5\n",
      "zero length feature at index: 23\n",
      "zero length feature at index: 45\n",
      "zero length feature at index: 138\n",
      "zero length feature at index: 148\n",
      "zero length feature at index: 165\n",
      "zero length feature at index: 238\n",
      "zero length feature at index: 239\n",
      "zero length feature at index: 250\n",
      "zero length feature at index: 281\n",
      "zero length feature at index: 282\n",
      "zero length feature at index: 295\n",
      "zero length feature at index: 306\n",
      "zero length feature at index: 307\n",
      "zero length feature at index: 322\n",
      "zero length feature at index: 333\n",
      "zero length feature at index: 363\n",
      "zero length feature at index: 364\n",
      "zero length feature at index: 369\n",
      "zero length feature at index: 382\n",
      "zero length feature at index: 399\n",
      "zero length feature at index: 405\n",
      "zero length feature at index: 409\n",
      "zero length feature at index: 451\n",
      "zero length feature at index: 455\n",
      "zero length feature at index: 462\n",
      "zero length feature at index: 528\n",
      "zero length feature at index: 553\n",
      "zero length feature at index: 570\n",
      "zero length feature at index: 571\n",
      "zero length feature at index: 628\n",
      "zero length feature at index: 654\n",
      "zero length feature at index: 658\n",
      "zero length feature at index: 720\n",
      "zero length feature at index: 727\n",
      "zero length feature at index: 736\n",
      "zero length feature at index: 740\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m train_features \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(maxFeatures):\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;66;03m# extract a feature\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m     train_feature \u001B[38;5;241m=\u001B[39m \u001B[43mhelper_feature_extraction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mads_train_ubm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m[\u001B[49m\u001B[43mf\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormFactors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(train_feature) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     10\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzero length feature at index: \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(f))\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mhelper_feature_extraction\u001B[1;34m(raw_audio_file, norm)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# detect speech => convert this into one function\u001B[39;00m\n\u001B[0;32m     20\u001B[0m v \u001B[38;5;241m=\u001B[39m vad\u001B[38;5;241m.\u001B[39mVoiceActivityDetector(signal_rate, signal, \u001B[38;5;241m0.5\u001B[39m)\n\u001B[1;32m---> 21\u001B[0m detected_speach \u001B[38;5;241m=\u001B[39m \u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetect_speech\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m idx \u001B[38;5;241m=\u001B[39m v\u001B[38;5;241m.\u001B[39mconvert_windows_to_readible_labels(detected_speach)\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m idx:\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\pythonProjects\\ubm\\misc\\vad.py:156\u001B[0m, in \u001B[0;36mVoiceActivityDetector.detect_speech\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sample_end \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(data): sample_end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(data) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    155\u001B[0m data_window \u001B[38;5;241m=\u001B[39m data[sample_start:sample_end]\n\u001B[1;32m--> 156\u001B[0m energy_freq \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_calculate_normalized_energy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_window\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    157\u001B[0m sum_voice_energy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sum_energy_in_band(energy_freq, start_band, end_band)\n\u001B[0;32m    158\u001B[0m sum_full_energy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(energy_freq\u001B[38;5;241m.\u001B[39mvalues())\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\pythonProjects\\ubm\\misc\\vad.py:61\u001B[0m, in \u001B[0;36mVoiceActivityDetector._calculate_normalized_energy\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_calculate_normalized_energy\u001B[39m(\u001B[38;5;28mself\u001B[39m, data):\n\u001B[0;32m     60\u001B[0m     data_freq \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_calculate_frequencies(data)\n\u001B[1;32m---> 61\u001B[0m     data_energy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_calculate_energy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;66;03m# data_energy = self._znormalize_energy(data_energy) #znorm brings worse results\u001B[39;00m\n\u001B[0;32m     63\u001B[0m     energy_freq \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connect_energy_with_frequencies(data_freq, data_energy)\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\pythonProjects\\ubm\\misc\\vad.py:42\u001B[0m, in \u001B[0;36mVoiceActivityDetector._calculate_energy\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_calculate_energy\u001B[39m(\u001B[38;5;28mself\u001B[39m, data):\n\u001B[1;32m---> 42\u001B[0m     data_amplitude \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_calculate_amplitude\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m     data_energy \u001B[38;5;241m=\u001B[39m data_amplitude \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m     44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data_energy\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\pythonProjects\\ubm\\misc\\vad.py:37\u001B[0m, in \u001B[0;36mVoiceActivityDetector._calculate_amplitude\u001B[1;34m(self, audio_data)\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_calculate_amplitude\u001B[39m(\u001B[38;5;28mself\u001B[39m, audio_data):\n\u001B[1;32m---> 37\u001B[0m     data_ampl \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mabs(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfft\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfft\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio_data\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     38\u001B[0m     data_ampl \u001B[38;5;241m=\u001B[39m data_ampl[\u001B[38;5;241m1\u001B[39m:]\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data_ampl\n",
      "File \u001B[1;32m<__array_function__ internals>:5\u001B[0m, in \u001B[0;36mfft\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ubm\\lib\\site-packages\\numpy\\fft\\_pocketfft.py:215\u001B[0m, in \u001B[0;36mfft\u001B[1;34m(a, n, axis, norm)\u001B[0m\n\u001B[0;32m    213\u001B[0m     n \u001B[38;5;241m=\u001B[39m a\u001B[38;5;241m.\u001B[39mshape[axis]\n\u001B[0;32m    214\u001B[0m inv_norm \u001B[38;5;241m=\u001B[39m _get_forward_norm(n, norm)\n\u001B[1;32m--> 215\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43m_raw_fft\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minv_norm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ubm\\lib\\site-packages\\numpy\\fft\\_pocketfft.py:70\u001B[0m, in \u001B[0;36m_raw_fft\u001B[1;34m(a, n, axis, is_real, is_forward, inv_norm)\u001B[0m\n\u001B[0;32m     67\u001B[0m         a \u001B[38;5;241m=\u001B[39m z\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;241m==\u001B[39m a\u001B[38;5;241m.\u001B[39mndim\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 70\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mpfi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_real\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_forward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfct\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     a \u001B[38;5;241m=\u001B[39m swapaxes(a, axis, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# [3] gmm ubm EM\n",
    "\n",
    "# 3.a collect train features\n",
    "maxFeatures = len(ads_train_ubm.files) # len(\n",
    "train_features = []\n",
    "for f in range(maxFeatures):\n",
    "    # extract a feature\n",
    "    train_feature = helper_feature_extraction(ads_train_ubm.files[f], normFactors)\n",
    "    if len(train_feature) == 0:\n",
    "        print('zero length feature at index: ' + str(f))\n",
    "    else:\n",
    "        train_features.append(train_feature)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# 3.b create UBM using sklearn\n",
    "\n",
    "from sklearn.mixture import GaussianMixture, _base, _gaussian_mixture\n",
    "\n",
    "numComponents = 32\n",
    "alpha = np.ones(numComponents) / numComponents\n",
    "numFeatures = normalised_features[0].shape[1]\n",
    "mu = np.random.random((numComponents,numFeatures))\n",
    "sigma = np.random.random((numComponents,numFeatures))\n",
    "\n",
    "# TODO function to init GMM\n",
    "ubm = GaussianMixture(n_components=numComponents, covariance_type='diag', weights_init=alpha, means_init=mu, precisions_init=sigma)\n",
    "\n",
    "ubm.means_ = mu\n",
    "ubm.covariances_ = sigma\n",
    "ubm.weights_ = alpha\n",
    "ubm.precisions_cholesky_ = _gaussian_mixture._compute_precision_cholesky(sigma, 'diag')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, log likelihood -6664.798061316596\n",
      "iter: 10, log likelihood -5278.89807991309\n",
      "iter: 20, log likelihood -5226.25744729094\n",
      "iter: 30, log likelihood -5205.135872586785\n",
      "iter: 40, log likelihood -5193.571263263209\n",
      "iter: 50, log likelihood -5185.511126132337\n",
      "iter: 60, log likelihood -5181.354714833754\n",
      "iter: 70, log likelihood -5177.664894153763\n",
      "iter: 80, log likelihood -5173.590992279247\n",
      "iter: 90, log likelihood -5170.6458312371715\n"
     ]
    }
   ],
   "source": [
    "# 3.c em algorithm #todo something not right here double check\n",
    "\n",
    "maxIter = 100\n",
    "targetLogLikelihood = 0\n",
    "tol = 1e-3\n",
    "pastL = -np.inf\n",
    "\n",
    "for i in range(maxIter):\n",
    "\n",
    "    # E step\n",
    "    N = np.zeros((1, numComponents))\n",
    "    F = np.zeros((numFeatures, numComponents))\n",
    "    S = np.zeros((numFeatures, numComponents))\n",
    "    L = 0\n",
    "\n",
    "\n",
    "    for feature in train_features:\n",
    "        # check for sub par samples\n",
    "        if len(feature) > 0:\n",
    "            logLikelihood = ubm._estimate_weighted_log_prob(feature)\n",
    "            logLikelihoodSum = logsumexp(logLikelihood, axis=1)\n",
    "            gamma = np.exp((logLikelihood.T - logLikelihoodSum)).T\n",
    "\n",
    "            # Compute Baum-Welch stats\n",
    "            n = np.sum(gamma,axis=0)\n",
    "            f = np.dot(feature.T, gamma)\n",
    "            s = np.dot(feature.T * feature.T, gamma)\n",
    "\n",
    "            N = N + n\n",
    "            F = F + f\n",
    "            S = S + s\n",
    "\n",
    "            L = L + np.sum(logLikelihoodSum)\n",
    "\n",
    "    # M step\n",
    "    L = L / len(train_features)\n",
    "    if i%10 == 0:\n",
    "        print('iter: ' + str(i) + ', log likelihood ' + str(L))\n",
    "    if L > targetLogLikelihood or np.abs(pastL - L) < tol:\n",
    "        print('converged on iter: ' + str(i))\n",
    "        ubm.converged_ = True\n",
    "        break\n",
    "    else:\n",
    "        pastL = L\n",
    "\n",
    "    # TODO create function out of this\n",
    "    N = np.maximum(N, eps)\n",
    "    weights = np.maximum(N / np.sum(N), eps)\n",
    "    newWeights = weights / np.sum(weights)  # why again, because of eps ...\n",
    "    newMeans = F / N\n",
    "    newCovariances = np.maximum((S / N) - np.square(ubm.means_).T, eps)\n",
    "    ubm.weights_ = np.squeeze(newWeights.T)\n",
    "    ubm.means_ = newMeans.T\n",
    "    ubm.covariances_ = newCovariances.T\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# sklearn does better - probably because of k means init\u001B[39;00m\n\u001B[0;32m      3\u001B[0m ubm_test \u001B[38;5;241m=\u001B[39m GaussianMixture(n_components\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m, covariance_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiag\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m train_features_flattened \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([item \u001B[38;5;28;01mfor\u001B[39;00m sublist \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtrain_features\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m sublist])\n\u001B[0;32m      5\u001B[0m ubm_test\u001B[38;5;241m.\u001B[39mfit(train_features_flattened)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msklearn model\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(ubm_test\u001B[38;5;241m.\u001B[39mscore(train_features[\u001B[38;5;241m0\u001B[39m])))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_features' is not defined"
     ]
    }
   ],
   "source": [
    "# sklearn does better - probably because of k means init\n",
    "\n",
    "ubm_test = GaussianMixture(n_components=32, covariance_type='diag')\n",
    "train_features_flattened = np.array([item for sublist in train_features for item in sublist])\n",
    "ubm_test.fit(train_features_flattened)\n",
    "print('sklearn model' + str(ubm_test.score(train_features[0])))\n",
    "# print('my model' + str(ubm.score(train_features[0])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# saving sklearns version for the moment\n",
    "pickle.dump(ubm_test, open('ubm.pickle','wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "try:\n",
    "    ubm = pickle.load(open('ubm.pickle','rb'))\n",
    "except:\n",
    "    pickle.dump(ubm, open('ubm.pickle','wb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker enrollment completed in: \n"
     ]
    }
   ],
   "source": [
    "# [4] map enroll\n",
    "import helper_functions as hf\n",
    "reload(hf)\n",
    "\n",
    "# do stuff\n",
    "numComponents = 32\n",
    "numFeatures = 13\n",
    "relevanceFactor = 16\n",
    "speakers = np.unique(ads_enroll.labels)\n",
    "numSpeakers = len(speakers)\n",
    "gmmCellArray = []\n",
    "\n",
    "for i in range(len(speakers)):\n",
    "    adsTrainSubset = myads.subset(ads_enroll, speakers[i])\n",
    "    N = np.zeros((1, numComponents))\n",
    "    F = np.zeros((numFeatures, numComponents))\n",
    "    S = np.zeros((numFeatures, numComponents))\n",
    "\n",
    "    for file in adsTrainSubset.files:\n",
    "        speaker_feature = helper_feature_extraction(file, normFactors)\n",
    "        if len(speaker_feature) > 0:\n",
    "            # BW stats, # TODO make this in to function\n",
    "            n,f,s,l = hf.helper_expectation(speaker_feature, ubm)\n",
    "            N = N + n\n",
    "            F = F + f\n",
    "            S = S + s\n",
    "        else:\n",
    "            print('skipping train file because len = 0')\n",
    "\n",
    "    gmm = hf.helper_maximization(N,F,S,numComponents)\n",
    "\n",
    "    alpha = N / (N + relevanceFactor)\n",
    "\n",
    "    mu = (alpha.T*gmm.means_) + ((1-alpha).T*ubm.means_)\n",
    "    gmm.means_ = mu\n",
    "\n",
    "    # gmm.covariances_ = alpha*(S/N) + (1-alpha)*(ubm.covariances_ + np.square(ubm.means_)).T - np.square(gmm.means_).T\n",
    "    # gmm.covariances_ = np.maximum(gmm.covariances_,eps).T\n",
    "    #\n",
    "    # gmm.weights_ = alpha*(N/np.sum(N)) + (1-alpha)*ubm.weights_.T\n",
    "    # gmm.weights_ = np.squeeze(gmm.weights_/np.sum(gmm.weights_))\n",
    "\n",
    "    # gmm.precisions_cholesky_ = _gaussian_mixture._compute_precision_cholesky(gmm.covariances_, 'diag')\n",
    "\n",
    "    gmmCellArray.append(gmm)\n",
    "\n",
    "print('speaker enrollment completed in: ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-21.432223860353925 -25.29854786690323\n"
     ]
    }
   ],
   "source": [
    "# quick test\n",
    "adsTrainSubset = myads.subset(ads_enroll, speakers[0])\n",
    "speaker_feature = helper_feature_extraction(adsTrainSubset.files[1], normFactors)\n",
    "adapted_score = gmmCellArray[0].score(speaker_feature)\n",
    "ubm_score = ubm.score(speaker_feature)\n",
    "print(adapted_score, ubm_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "enrolled_gmms = {}\n",
    "for i in range(len(gmmCellArray)):\n",
    "    enrolled_gmms[speakers[i]] = gmmCellArray[i]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# false rejection rate -\n",
    "# The speaker false rejection rate (FRR) is the rate that a given speaker is incorrectly rejected. Use the known speaker set to determine the speaker false rejection rate for a set of thresholds.\n",
    "\n",
    "llr = []\n",
    "\n",
    "for i in range(len(speakers)):\n",
    "    cur_speaker = speakers[i]\n",
    "    local_gmm = enrolled_gmms[cur_speaker]\n",
    "    adsTestSubset = myads.subset(ads_test, cur_speaker)\n",
    "    llrPerSpeaker = np.zeros(len(adsTestSubset.files))\n",
    "\n",
    "    for y in range(len(adsTestSubset.files)):\n",
    "        file = adsTestSubset.files[y]\n",
    "        speaker_feature = helper_feature_extraction(file, normFactors)\n",
    "\n",
    "        logLikelihood = local_gmm._estimate_weighted_log_prob(speaker_feature)\n",
    "        lspeaker = logsumexp(logLikelihood, axis=1)\n",
    "\n",
    "        logLikelihood = ubm._estimate_weighted_log_prob(speaker_feature)\n",
    "        lubm = logsumexp(logLikelihood, axis=1)\n",
    "\n",
    "        llrPerSpeaker[y] = np.mean(lspeaker-lubm)\n",
    "\n",
    "    llr.append(llrPerSpeaker)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}